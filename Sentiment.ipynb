{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "367f59bf",
      "metadata": {
        "id": "367f59bf"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "# from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCucB3q_cdin",
        "outputId": "ce5d4c59-4662-4846-b27c-316cb4bef5b9"
      },
      "id": "tCucB3q_cdin",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9e55f24c",
      "metadata": {
        "id": "9e55f24c"
      },
      "outputs": [],
      "source": [
        "tweets = pd.read_csv(\"./Tweets.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdcfe2fc",
      "metadata": {
        "id": "bdcfe2fc"
      },
      "outputs": [],
      "source": [
        "tweets.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2682a444",
      "metadata": {
        "id": "2682a444"
      },
      "outputs": [],
      "source": [
        "tweets.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a24efdb",
      "metadata": {
        "id": "7a24efdb"
      },
      "outputs": [],
      "source": [
        "tweets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "912fee14",
      "metadata": {
        "id": "912fee14"
      },
      "outputs": [],
      "source": [
        "# #this\n",
        "# review_dataset[\"tweet_coord\"].dropna()\n",
        "# # or this\n",
        "# review_dataset[\"tweet_coord\"][~review_dataset[\"tweet_coord\"].isnull()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b02074d9",
      "metadata": {
        "id": "b02074d9"
      },
      "outputs": [],
      "source": [
        "tweets.text.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b962c8",
      "metadata": {
        "id": "98b962c8"
      },
      "source": [
        "### Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68b7c8d9",
      "metadata": {
        "id": "68b7c8d9"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "# Building a word cloud for negative tweet text\n",
        "negative_tweets = tweets[\"text\"][tweets[\"airline_sentiment\"] ==\"negative\"]\n",
        "joined_negative_tweets = \"\".join(negative_tweets)\n",
        "\n",
        "my_stop_words = set(STOPWORDS)\n",
        "# reg = r\"@\"|\n",
        "my_stop_words = my_stop_words.union([\"@VirginAmerica\", \"united\",\"flight\",\"AmericanAir\",\"USAirways\",\"SouthwestAir\",\"JetBlue\"])\n",
        "word_cloud = WordCloud(stopwords=my_stop_words, regexp=r'\\b[^\\d\\W][^\\d\\W]+\\b').generate(joined_negative_tweets)\n",
        "\n",
        "#show word cloud\n",
        "plt.imshow(word_cloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efbb2542",
      "metadata": {
        "id": "efbb2542"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "# Building a word cloud for negative tweet text\n",
        "# positive_neutral_tweets = tweets[\"text\"][~tweets[\"airline_sentiment\"] ==\"negative\"]\n",
        "positive_neutral_tweets = tweets[\"text\"][tweets[\"airline_sentiment\"] !=\"negative\"]\n",
        "joined_positive_neutral_tweets = \"\".join(positive_neutral_tweets)\n",
        "\n",
        "my_stop_words = set(STOPWORDS)\n",
        "# reg = r\"@\"|\n",
        "my_stop_words = my_stop_words.union([\"@VirginAmerica\", \"united\",\"flight\",\"AmericanAir\",\"USAirways\",\"SouthwestAir\", \"JetBlue\",\"VirginAmerica\",\"co\", \"t\"])\n",
        "word_cloud = WordCloud(background_color=\"white\", stopwords=my_stop_words, regexp=r'\\b[^\\d\\W][^\\d\\W]+\\b').generate(joined_positive_neutral_tweets)\n",
        "\n",
        "#show word cloud\n",
        "plt.imshow(word_cloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e3cc4b0",
      "metadata": {
        "id": "8e3cc4b0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "723fe039",
      "metadata": {
        "id": "723fe039"
      },
      "source": [
        "### Data Preprocessing and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "31c5788d",
      "metadata": {
        "id": "31c5788d"
      },
      "outputs": [],
      "source": [
        "tweets = tweets[[\"text\",\"airline_sentiment\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b875787d",
      "metadata": {
        "id": "b875787d"
      },
      "outputs": [],
      "source": [
        "#Encode the target label\n",
        "from sklearn.preprocessing  import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "tweets.airline_sentiment = le.fit_transform(tweets.airline_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f5a48d1d",
      "metadata": {
        "id": "f5a48d1d"
      },
      "outputs": [],
      "source": [
        "# Defie the text preprocessor function\n",
        "def preprocessor(text):\n",
        "    text = re.sub(r\"@[d W]+\", \"\", text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "56f41401",
      "metadata": {
        "id": "56f41401"
      },
      "outputs": [],
      "source": [
        "# Cleaning the text\n",
        "tweets.text = tweets.text.apply(preprocessor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "148113e7",
      "metadata": {
        "id": "148113e7"
      },
      "outputs": [],
      "source": [
        "# skip this\n",
        "# # Add text_length feature\n",
        "# from nltk import word_tokenize\n",
        "# tokens = [word_tokenize(word) for word in tweets.text]\n",
        "# length =[]\n",
        "# for i in range(len(tokens)):\n",
        "#     length.append(len(tokens[i]))\n",
        "\n",
        "# tweets[\"text_length\"] = length"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8bd07b",
      "metadata": {
        "id": "6d8bd07b"
      },
      "source": [
        "## Building the Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "beb087c9",
      "metadata": {
        "id": "beb087c9"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#defining the stop words\n",
        "stop =  set(stopwords.words(\"english\"))\n",
        "stop_words = stop\n",
        "stop_words_2 = stop.union({\"AmericanAir\",\"USAirways\",\"SouthwestAir\", \"JetBlue\",\"VirginAmerica\"})\n",
        "\n",
        "def tokenizer(text):\n",
        "    return [word_tokenize(text)]\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [PorterStemmer().stem(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "acba89f3",
      "metadata": {
        "id": "acba89f3"
      },
      "outputs": [],
      "source": [
        "# Define X and y\n",
        "y = tweets.airline_sentiment\n",
        "X = tweets.drop('airline_sentiment', axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "54c5037a",
      "metadata": {
        "id": "54c5037a"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=456)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "544f95d9",
      "metadata": {
        "id": "544f95d9"
      },
      "source": [
        "#### Traing a Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "iZYFAxo1gZod",
        "outputId": "81a4c3e1-a0bc-4f03-9479-1400806b36ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "iZYFAxo1gZod",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11712, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "83eb98b8",
      "metadata": {
        "id": "83eb98b8",
        "outputId": "54c00eda-d7d0-4d30-be2c-412e6e5e0b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=0.1, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer_porter at 0x79120f69da20>; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=1.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n",
            "[CV] END clf__C=10.0, clf__penalty=l1, clf__solver=saga, vect__ngram_range=(1, 2), vect__norm=None, vect__stop_words={'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'having', 'your', 'until', 'what', \"hadn't\", 'about'}, vect__tokenizer=<function tokenizer at 0x79120f97af80>, vect__use_idf=False; total time=   0.0s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\nAll the 160 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n20 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1372, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1261, in _count_vocab\n    feature_idx = vocabulary[feature]\nTypeError: unhashable type: 'list'\n\n--------------------------------------------------------------------------------\n8 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 473, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1223, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1320, in check_X_y\n    check_consistent_length(X, y)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 457, in check_consistent_length\n    raise ValueError(\nValueError: Found input variables with inconsistent numbers of samples: [1, 9369]\n\n--------------------------------------------------------------------------------\n12 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 473, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1223, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1320, in check_X_y\n    check_consistent_length(X, y)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 457, in check_consistent_length\n    raise ValueError(\nValueError: Found input variables with inconsistent numbers of samples: [1, 9370]\n\n--------------------------------------------------------------------------------\n80 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n    estimator._validate_params()\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'about', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'too', 'hasn', 'just', 'your', 'until', 'what', \"hadn't\", 'having'} instead.\n\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n    estimator._validate_params()\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'about', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before', \"won't\", \"doesn't\", 'below', 'now', \"isn't\", \"shouldn't\", \"shan't\", 'then', 'SouthwestAir', 'too', 'hasn', 'just', 'your', 'until', 'what', \"hadn't\", 'having'} instead.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-3c61d2516df6>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mgs_lr_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mgs_lr_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    994\u001b[0m                     )\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             )\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 160 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n20 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1372, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1261, in _count_vocab\n    feature_idx = vocabulary[feature]\nTypeError: unhashable type: 'list'\n\n--------------------------------------------------------------------------------\n8 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 473, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1223, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1320, in check_X_y\n    check_consistent_length(X, y)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 457, in check_consistent_length\n    raise ValueError(\nValueError: Found input variables with inconsistent numbers of samples: [1, 9369]\n\n--------------------------------------------------------------------------------\n12 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 473, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1223, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1320, in check_X_y\n    check_consistent_length(X, y)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 457, in check_consistent_length\n    raise ValueError(\nValueError: Found input variables with inconsistent numbers of samples: [1, 9370]\n\n--------------------------------------------------------------------------------\n80 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n    estimator._validate_params()\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'about', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", 'between', 'yourself', 'is', 'an', 'only', 'a', 'before'...\n\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n    estimator._validate_params()\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'again', 'after', 'same', 'up', 'or', 'its', 'very', 'd', 'further', \"you're\", 'through', \"she's\", 'shouldn', 'more', 'all', 'haven', 'his', 'under', 'some', 'needn', 'doing', 'once', 'our', 'are', 'does', 'whom', 'because', 'the', 'while', 'those', 'over', 'did', 'you', 'yours', 'their', 'each', 'no', 'ma', 'were', 'any', \"it's\", 'will', \"should've\", 'o', 'why', 's', 'down', 'mightn', \"you'd\", \"mightn't\", \"you'll\", 'this', 'on', \"haven't\", 'to', 'shan', \"hasn't\", 'aren', 'other', 'not', 't', 'mustn', 'her', 'and', 'being', 'couldn', 'didn', 'from', 'for', 'nor', 'there', 'into', 'don', 'how', 'have', 'but', 'it', 'itself', 'am', 'she', 'be', 'with', 'hadn', 'do', 'of', 'off', \"weren't\", 'i', 'out', 'myself', 'isn', 'at', 'if', \"aren't\", 'ours', \"needn't\", 'he', 'such', 'themselves', \"don't\", 'll', 'wouldn', \"that'll\", 'AmericanAir', 'that', 'had', 'him', 'own', 'has', 'me', 'against', \"mustn't\", 'hers', 'here', 'where', 've', 'they', 'so', 'won', 'VirginAmerica', \"wouldn't\", 'them', 'these', 'ain', 'been', 'by', 'wasn', 'theirs', 'about', 'herself', 'himself', \"didn't\", \"couldn't\", 'when', 'above', 're', 'can', 'who', 'most', 'weren', 'we', 'doesn', 'both', 'should', 'JetBlue', 'during', 'was', 'my', 'm', 'ourselves', 'as', 'yourselves', 'than', 'USAirways', 'few', \"wasn't\", 'y', 'in', 'which', \"you've\", ...\n"
          ]
        }
      ],
      "source": [
        "from  sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# tfidf = ColumnTransformer([(\"vectorizer\", TfidfVectorizer(lowercase=False),0), (\"nothing\", \"passthrough\",[1])] )\n",
        "tfidf = TfidfVectorizer(lowercase=False)\n",
        "\n",
        "\n",
        "param_grid =[\n",
        "    {\n",
        "        \"vect__ngram_range\" : [(1,1),(1,2)],\n",
        "        \"vect__stop_words\" : [None, stop_words],\n",
        "        \"vect__tokenizer\" : [tokenizer, tokenizer_porter],\n",
        "        \"clf__penalty\" : [\"l2\"],\n",
        "        \"clf__C\" : [0.1, 10.0]\n",
        "    },\n",
        "    {\n",
        "        \"vect__ngram_range\" : [(1,1),(1,2)],\n",
        "        \"vect__stop_words\" : [stop_words, stop_words_2],\n",
        "        \"vect__tokenizer\" : [tokenizer],\n",
        "        \"vect__use_idf\" : [False],\n",
        "        \"vect__norm\" : [None],\n",
        "        \"clf__penalty\" : [\"l2\",\"l1\"],\n",
        "        \"clf__solver\" : [\"saga\"],\n",
        "        \"clf__C\" : [1.0, 10.0]\n",
        "\n",
        "    }\n",
        "]\n",
        "\n",
        "lr_tfidf = Pipeline([(\"vect\", tfidf), (\"clf\", LogisticRegression())])\n",
        "\n",
        "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=2, n_jobs=1)\n",
        "\n",
        "gs_lr_tfidf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "9bf4d517",
      "metadata": {
        "id": "9bf4d517",
        "outputId": "7f21a849-6f93-4d1e-d162-4a09117ff483",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__vectorizer__ngram_range': (1, 1), 'vect__vectorizer__stop_words': None, 'vect__vectorizer__tokenizer': <function tokenizer_porter at 0x791211f6ec20>}\n",
            "0.6420760934418456\n"
          ]
        }
      ],
      "source": [
        "print(gs_lr_tfidf.best_params_)\n",
        "print(gs_lr_tfidf.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abfd79b5",
      "metadata": {
        "id": "abfd79b5"
      },
      "outputs": [],
      "source": [
        "y_predicted = gs_lr_tfidf.best_estimator_.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdcef2fa",
      "metadata": {
        "id": "cdcef2fa"
      },
      "outputs": [],
      "source": [
        "gs_lr_tfidf.best_estimator_.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ded2824",
      "metadata": {
        "id": "5ded2824"
      },
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
        "\n",
        "# # Build the vectorizer\n",
        "# vect = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 2), max_features=200,\n",
        "#                     token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b').fit(tweets.text)\n",
        "# # Create sparse matrix from the vectorizer\n",
        "# X_tfidf = vect.transform(tweets.text)\n",
        "\n",
        "# # Create a DataFrame\n",
        "# X_tfidf_transformed = pd.DataFrame(X_tfidf.toarray(), columns=vect.get_feature_names_out())\n",
        "# X_tfidf_transformed.head()\n",
        "# # can also do this: max_df= 0.9, min_df=0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e4c6202",
      "metadata": {
        "id": "4e4c6202"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53893cc",
      "metadata": {
        "id": "f53893cc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02974911",
      "metadata": {
        "id": "02974911"
      },
      "outputs": [],
      "source": [
        "tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc9accc",
      "metadata": {
        "id": "dbc9accc"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([ X_tfidf_transformed, tweets], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7e24cda",
      "metadata": {
        "id": "f7e24cda"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb1eb54",
      "metadata": {
        "id": "5fb1eb54"
      },
      "outputs": [],
      "source": [
        "# Lastly, build the classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa72edfd",
      "metadata": {
        "id": "aa72edfd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2666191d",
      "metadata": {
        "id": "2666191d"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Train a logistic regression\n",
        "log_reg = LogisticRegression(solver=\"newton-cg\", C= 1).fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels\n",
        "y_predicted = log_reg.predict(X_test)\n",
        "\n",
        "# Print accuracy score and confusion matrix on test set\n",
        "print('Accuracy on the test set: ', accuracy_score(y_test, y_predicted))\n",
        "# print(confusion_matrix(y_test, y_predicted)/len(y_test))\n",
        "print(confusion_matrix(y_test, y_predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c203e95",
      "metadata": {
        "id": "7c203e95"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}